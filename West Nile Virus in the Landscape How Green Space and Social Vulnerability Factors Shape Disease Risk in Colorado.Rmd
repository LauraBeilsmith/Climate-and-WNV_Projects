---
title: "EPID 7656 Final Paper"
output: html_document
---

#  Appendix A: Code
#  West Nile Virus in the Landscape: How Green Space and Social Vulnerability Factors Shape Disease Risk in Colorado
#### Laura Beilsmith
#### University of Colorado School of Public Health
#### May 15, 2025



```{r setup, include=FALSE}

#load packages 
library(tidyverse)
library(tidyr)
library(dplyr)
library(sf)
library(tmap)
library(tmaptools)
library(ggplot2)
library(lubridate)
library(RAQSAPI)
library(ggplot2)
library(tibble)
library(knitr)
library(kableExtra)
library(broom)
library(OpenStreetMap)
library(leaflet)
library(spaMM)
library(stars)
library(tidycensus)
library(stringr)
library(doParallel)
library(foreach)
library(geodata)
library(readr)

```

################################################################################
# READING IN AND CLEANING DATA
################################################################################

#### Loading In and Cleaning the WNV, SVI, and NLCD Data

```{r loadinWNVdata}

#file path for WNV data
wnv_data_path_CO <- "C:/Users/Laura_2/Documents/EPID 7656 Enviro Data Science/Final Project/WNVdata/"

#study years corresponding to files 
wnv_years_CO <- c(2011, 2013, 2015, 2017, 2019, 2021)

#start list
wnv_list_CO <- list()

#Loop through and load each year
for (year in wnv_years_CO) {
  file_name_CO <- str_c("West Nile virus human and non-human activity by county ", year, ".csv")
  file_path_CO <- file.path(wnv_data_path_CO, file_name_CO)
  
  if (file.exists(file_path_CO)) {
    data_CO <- read_csv(file_path_CO, show_col_types = FALSE) %>%
      mutate(
        County = as.character(County),  # <--- fix type issue here
        year = as.character(year)
      )
    
    wnv_list_CO[[as.character(year)]] <- data_CO
  } else {
    message("File for year ", year, " is missing.")  ##just in case one of them doesn't show up or has a different name
  }
}

#combine all years into one dataframe
wnv_combined_data_CO <- bind_rows(wnv_list_CO, .id = "year")

#look at it 
head(wnv_combined_data_CO)


```


#### read in NLCD 
```{r readinNLCD}


#file path define 
nlcd_data_path_EDS <- "C:/Users/Laura_2/Documents/EPID 7656 Enviro Data Science/Final Project/NLCDdata/"

#years for NLCD data
nlcd_years_EDS <- c(2011, 2013, 2015, 2017, 2019, 2021)

#initialize a list to store data
nlcd_list_CO <- list()

#load each NLCD dataset and filter to Colorado (GEOID or GISJOIN starting with "G08")
for (year in nlcd_years_EDS) {
  file_name <- str_c("nhgis_cty_sub2010_tl2020_nlcd", year, ".csv")
  file_path <- file.path(nlcd_data_path_EDS, file_name)
  
  if (file.exists(file_path)) {
    data <- read_csv(file_path, show_col_types = FALSE)
    
    # Filter to Colorado counties using GISJOIN prefix
    data_CO <- data %>%
      filter(str_starts(GISJOIN, "G08"))
    
    nlcd_list_CO[[as.character(year)]] <- data_CO
  } else {
    message("File for year ", year, " is missing.")
  }
}

# combine all Colorado NLCD data into one dataframe
nlcd_combined_data_CO <- bind_rows(nlcd_list_CO, .id = "year")

#look at it to verify it read in right
head(nlcd_combined_data_CO)



```


#### clean NLCD data into a dataframe
```{r cleaningNLCDdata}


cleaned_nlcd_data_CO <- nlcd_combined_data_CO %>%
  filter(str_starts(GISJOIN, "G08")) %>%
  mutate(
    year = as.character(year),
    
    # Greenspace: sum of natural land cover proportions
    greenspace = (PROP_41 + PROP_42 + PROP_43 + PROP_71 + PROP_90 + PROP_95) * 100,
    
    # Urban: sum of developed land proportions
    NCLD_urban = (PROP_22 + PROP_23 + PROP_24) * 100
  ) %>%
  dplyr::select(
    year,
    GISJOIN,
    GEOID,
    GEOGYEAR,
    NLCDYEAR,
    NCLD_urban,
    greenspace
  )

#check the cleaned dataset both structure and look at it to verify the GEOID now looks like FIPS codes in other datasets 
#so that we can merge later
head(cleaned_nlcd_data_CO)
str(cleaned_nlcd_data_CO)




#prep NLCD for merge
nlcd_by_year_CO <- cleaned_nlcd_data_CO %>%
  mutate(
    County_FIPS = str_pad(as.character(GEOID), width = 5, pad = "0")
  ) %>%
  dplyr::select(County_FIPS, year, greenspace)


```



#### reading in SVI data 

```{r readinSVIdata}


#file path for SVI data
svi_data_path_CO <- "C:/Users/Laura_2/Documents/EPID 7656 Enviro Data Science/Final Project/SVIData/"

#study years
svi_years_CO <- c(2011, 2013, 2015, 2017, 2019, 2021)

#initialize the list for each year's data
svi_list_CO <- list()

#read in each file 
for (year in svi_years_CO) {
  file_name_CO <- str_c("Colorado_county_", year, ".csv")
  file_path_CO <- file.path(svi_data_path_CO, file_name_CO)
  
  if (file.exists(file_path_CO)) {
    svi_data_CO <- read_csv(file_path_CO, show_col_types = FALSE) %>%
      mutate(year = as.character(year))
    
    svi_list_CO[[as.character(year)]] <- svi_data_CO
  } else {
    message("File for year ", year, " is missing.")
  }
}

#combine all years into a single Colorado-only dataset
svi_combined_data_CO <- bind_rows(svi_list_CO, .id = "year")
head(svi_combined_data_CO)


#becauase the WNV data doesn't include counties if there were 0's in that county that year 
#so we need to make sure we maintain any counties because we're going to impute later 
colorado_counties <- unique(svi_combined_data_CO$FIPS)


```


#### reading in temperature data
##### downloaded from NOAA at a glance climate for CO by county 

```{r load_temp_data}

#file path and years
temp_data_path <- "C:/Users/Laura_2/Documents/EPID 7656 Enviro Data Science/Final Project/Data/NOAA_temp"
temp_years <- c(2011, 2013, 2015, 2017, 2019, 2021)

#initialize list
temp_list <- list()

#loop to read and process each file
for (year in temp_years) {
  file_name <- paste0("temp_", year, "_data.csv")
  file_path <- file.path(temp_data_path, file_name)

  if (file.exists(file_path)) {
    data <- read_csv(file_path, show_col_types = FALSE) %>%
      mutate(
        County_FIPS = str_pad(as.character(County_FIPS), width = 5, pad = "0"),
        year = as.character(year)
      ) %>%
      dplyr::select(County_FIPS, year, temp_value = Value)

    temp_list[[as.character(year)]] <- data
  } else {
    message("Missing temperature file for year ", year)
  }
}

#combine into single data frame
temp_combined_data <- bind_rows(temp_list)

#look at it 
head(temp_combined_data)
#units are Fahrenheit
#later rescale this by 5 I think for interpretability

```


#### reading in precip data

```{r load_precip_data}

#define file path and years
precip_data_path <- "C:/Users/Laura_2/Documents/EPID 7656 Enviro Data Science/Final Project/Data/NOAA_precip"
precip_years <- c(2011, 2013, 2015, 2017, 2019, 2021)

#initialize list
precip_list <- list()

#loop to read and process each file
for (year in precip_years) {
  file_name <- paste0("precip_", year, "_data.csv")
  file_path <- file.path(precip_data_path, file_name)

  if (file.exists(file_path)) {
    data <- read_csv(file_path, show_col_types = FALSE) %>%
      mutate(
        County_FIPS = str_pad(as.character(County_FIPS), width = 5, pad = "0"),
        year = as.character(year)
      ) %>%
      dplyr::select(County_FIPS, year, precip_value = Value)

    precip_list[[as.character(year)]] <- data
  } else {
    message("Missing precipitation file for year ", year)  #just to make sure we don't miss any files in the folders
  }
}

#combine into single data frame
precip_combined_data <- bind_rows(precip_list)
#look at it to verify it came in properly
head(precip_combined_data)
#units are inches for precip 

```



#### clean and impute WNV cases 

```{r cleanWNVdata}


#################################################################################
#imputation across all counties 
#any county with a 0 will get the median for that year imputed 
#so if a county is 0 in 2011, it's going to get imputed with the median value from 2011, etc


#full list ofstudy years 
colorado_years <- c("2011", "2013", "2015", "2017", "2019", "2021")

#create a full county-year grid (so we have each county for each year of study period just once)
full_grid <- expand_grid(
  County_FIPS = colorado_counties,
  year = colorado_years
) %>%
  mutate(County_FIPS = str_pad(as.character(County_FIPS), width = 5, pad = "0"),
         year = as.character(year))


#clean raw WNV data and join to full grid
cleaned_wnv_data_CO <- wnv_combined_data_CO %>%
  mutate(
    County_FIPS = str_pad(as.character(County), width = 5, pad = "0"),
    year = as.character(year)
  ) %>%
  filter(str_starts(County_FIPS, "08")) %>%
  dplyr::select(County_FIPS, year, `Reported human cases`) %>%
  group_by(County_FIPS, year) %>%
  summarize(reported_cases_raw = sum(`Reported human cases`, na.rm = TRUE), .groups = "drop")

#join to full grid and impute missing/zero with year-specific median
cleaned_wnv_data_CO <- full_grid %>%
  left_join(cleaned_wnv_data_CO, by = c("County_FIPS", "year")) %>%
  mutate(reported_cases = na_if(reported_cases_raw, 0)) %>%    #making them NAs
  group_by(year) %>%
  mutate(
    year_median_cases = median(reported_cases, na.rm = TRUE),      #replacing the NAs with median
    reported_cases = if_else(is.na(reported_cases), year_median_cases, reported_cases),
    reported_cases = round(reported_cases)
  ) %>%
  ungroup() %>%
  dplyr::select(County_FIPS, year, reported_cases)


#check it:
head(cleaned_wnv_data_CO)


```



#### let's merge it all together
#### all 3 datasets
```{r merging_it_all_together}

#clean and prep SVI data (population density and scores)
svi_by_year_CO <- svi_combined_data_CO %>%
  mutate(
    County_FIPS = str_pad(as.character(FIPS), width = 5, pad = "0"),
    population_density = E_TOTPOP / AREA_SQMI,   ###calculating pop density tot pop divided by area sq mi
    year = as.character(year)
  ) %>%
  dplyr::select(
    County_FIPS,
    year,
    svi_score = RPL_THEMES,  ###all RPL themes make up the SVI score
    ses_score = RPL_THEME1,   #in case we want SES alone later, but will probably get rid of this later on 
    urban_svi_score = RPL_THEME4,    ##added in case we wanted urban area proxy, but population density actually works better so we may delete later
    population_density
  )



## step 2 clean up NLCD data before merging
nlcd_by_year_CO <- cleaned_nlcd_data_CO %>%
  mutate(
    County_FIPS = substr(GEOID, 1, 5),
    year = as.character(year)
  ) %>%
  group_by(County_FIPS, year) %>%
  summarise(
    greenspace = mean(greenspace, na.rm = TRUE),
    .groups = "drop"
  )


#merge all datasets together by county and year
merged_by_year_CO <- cleaned_wnv_data_CO %>%
  left_join(svi_by_year_CO, by = c("County_FIPS", "year")) %>%
  left_join(nlcd_by_year_CO, by = c("County_FIPS", "year")) %>%
  left_join(temp_combined_data, by = c("County_FIPS", "year")) %>%
  left_join(precip_combined_data, by = c("County_FIPS", "year")) %>%
  arrange(County_FIPS, year)

#look at total row counts to verify all the merging worked as expected 
nrow(merged_by_year_CO)  #should be 384 (64 counties x 6 years)
colSums(is.na(merged_by_year_CO))  #checking for missing values in merged dataset
#look at the merged dataset to check 
head(merged_by_year_CO)


#remove any unnecessary variables
merged_by_year_CO <- merged_by_year_CO %>%
  dplyr::select(-ses_score, -urban_svi_score)


#check it 
head(merged_by_year_CO)



```



#### check for duplicates in any dataset
```{r check for duplicates}


#check for duplicates by look at counties and years and if any combination is repeated
cleaned_wnv_data_CO %>%
  count(County_FIPS, year) %>%
  filter(n > 1)

svi_by_year_CO %>%
  count(County_FIPS, year) %>%
  filter(n > 1)

nlcd_by_year_CO %>%
  count(County_FIPS, year) %>%
  filter(n > 1)

temp_combined_data %>%
  count(County_FIPS, year) %>%
  filter(n > 1)

precip_combined_data %>%
  count(County_FIPS, year) %>%
  filter(n > 1)

#should be a total of 384 rows (64 counties x 6 years)

```


#### per presentation feedback: 
#### adjust the temperature and population density scales to help with the tight CI's seen in the model later

```{r adjusting_variable_scale}

#rescale predictors to improve interpretability
merged_by_year_CO_adj <- merged_by_year_CO %>%
  mutate(
    temp_5F = temp_value / 5,              #temperature per 5°F
    pop_density_50 = population_density / 50, #population density per 50 people/sq mi
    greenspace_10 = greenspace / 10, #rescale greenspace so that it's per 10 percentage points
  )


#center SVI score, greenspace, population density per 50 per sq mi, 
#temp per 5 degrees F,  and precip in inches to help with interpretability 
merged_by_year_CO_adj <- merged_by_year_CO_adj %>%
  mutate(
    svi_score = scale(svi_score, scale = FALSE),
    greenspace = scale(greenspace_10, scale = FALSE), 
    pop_density_50 = scale(pop_density_50, scale = FALSE), 
    temp_5F = scale(temp_5F, scale = FALSE),
    precip_value = scale(precip_value, scale = FALSE)
  )

#show the dataset now that we have adjusted and rescaled everything
head(merged_by_year_CO_adj)


```

###################################################################################
#### finished cleaning dataset

#### All variables are now centered (and rescaled if needed)
#### greenspace = centered (per 10%)
#### pop_density = centered (per 50 ppl/sq mi)
#### temp_5F = centered (per 5°F)
#### precip = centered (per inch)
#### svi_score = centered (raw SVI score, 0 - 1)

##################################################################################


################################################################################
# STATISTICAL ANALYSIS
################################################################################


#### analysis - Poisson, testing overdispersion for negative binomial sincew we have count data as outcome 

```{r poisson_overdispersion_test}

poisson_model <- glm(reported_cases ~ svi_score + pop_density_50 + greenspace + temp_5F + precip_value,
                     family = poisson(link = "log"),
                     data = merged_by_year_CO_adj)


dispersion_stat <- sum(residuals(poisson_model, type = "pearson")^2) / poisson_model$df.residual
print(dispersion_stat)


#reslt is 11, indicating A LOT of overdispersion, indicating negative binomial will be best 

```

#### let's do negative binomial instead because we have a lot of overdispersion

```{r neg_binom}

library(MASS)
CO_nb_model <- glm.nb(
  reported_cases ~ svi_score + pop_density_50 + greenspace + temp_5F + precip_value,
  data = merged_by_year_CO_adj
)


summary(CO_nb_model)




library(broom)

tidy(CO_nb_model, exponentiate = TRUE, conf.int = TRUE)

```



#### model with interaction term

```{r}

library(MASS)

CO_nb_model_interaction <- glm.nb(
  reported_cases ~ pop_density_50 + temp_5F + precip_value +
    greenspace * svi_score,        #includes both main effects and interaction
  data = merged_by_year_CO_adj
)

#output with IRRs and 95% CIs
tidy(CO_nb_model_interaction, exponentiate = TRUE, conf.int = TRUE)


```


#### based on feedback from presentation, adding offset to model 
#### so that coefficients represent multiplicative changes in incidence rate (cases per person), which is more interpretable and comparable across counties.

```{r offset}

#grab E_totpop variable which is the total population per county and add it to the current working merged dataset
merged_by_year_CO_offset <- merged_by_year_CO_adj %>%
  left_join(
    svi_combined_data_CO %>% dplyr::select(FIPS, year, E_TOTPOP),
    by = c("County_FIPS" = "FIPS", "year" = "year")
)


CO_nb_model_interaction_offset <- glm.nb(
  reported_cases ~ pop_density_50 + temp_5F + precip_value +
    greenspace * svi_score + offset(log(E_TOTPOP)),
  data = merged_by_year_CO_offset
)


#output with IRRs and 95% CIs
tidy(CO_nb_model_interaction_offset, exponentiate = TRUE, conf.int = TRUE)

```

#### the above is the final model


#### effect modification with offset added into the model 

```{r EM_with_offset}

#split SVI into high and low categories (low means lower vulnerability, higher means greater vulnerability)

#create High/Low SVI group based on median of centered SVI score
merged_by_year_CO_offset_EM <- merged_by_year_CO_offset %>%
  mutate(
    svi_group = ifelse(svi_score > median(svi_score, na.rm = TRUE), "High SVI", "Low SVI"),
    svi_group = factor(svi_group, levels = c("Low SVI", "High SVI"))
  )

#split dataset into high and low 
low_svi_data_offset <- merged_by_year_CO_offset_EM %>% filter(svi_group == "Low SVI")
high_svi_data_offset <- merged_by_year_CO_offset_EM %>% filter(svi_group == "High SVI")

#fit the stratified negative binomial models *with offset*
nb_model_low_svi_offset <- glm.nb(
  reported_cases ~ greenspace + pop_density_50 + temp_5F + precip_value + offset(log(E_TOTPOP)),
  data = low_svi_data_offset
)

nb_model_high_svi_offset <- glm.nb(
  reported_cases ~ greenspace + pop_density_50 + temp_5F + precip_value + offset(log(E_TOTPOP)),
  data = high_svi_data_offset
)

#get IRRs with confidence intervals
low_svi_results_offset <- tidy(nb_model_low_svi_offset, exponentiate = TRUE, conf.int = TRUE)
high_svi_results_offset <- tidy(nb_model_high_svi_offset, exponentiate = TRUE, conf.int = TRUE)


#results
low_svi_results_offset
high_svi_results_offset

```

#################################################################################
##### finished with statistical analysis 

#### we have: 
#### a full model with an interaction term between SVI* greenspace and has been offset
#### a stratified model based on the median SVI score that have also been offset 

#################################################################################


################################################################################
# MAPS 
################################################################################

#### setting up mapping

```{r map_setup}

#load shapefile for Colorado counties
library(tigris)
options(tigris_use_cache = TRUE)

co_map_data <- counties(state = "CO", cb = TRUE, year = 2021) %>% #chose the state shapefiles for 2021 since it's the most recent and probably didn't change since 2011
  st_as_sf() %>%
  mutate(County_FIPS = GEOID)


#use co_map_data to join this data with the shapefiles to map incidence/greenspace/IRRs over CO

```


#### WNV map


```{r purpleWNVmap}

#make the WNV case counts map to assess spatial distribution over study period 

#summarize the total cases
wnv_summary <- merged_by_year_CO_adj %>%
  group_by(County_FIPS) %>%
  summarise(total_reported_cases_first = sum(reported_cases, na.rm = TRUE))

#join to spatial map of co counties created in an earlier chunk (Co_map_data)
co_map_data <- left_join(co_map_data, wnv_summary, by = "County_FIPS")

#plot the map with ggplot2, using purples (lighter = less, darker = more cases)
wnv_map_co <- ggplot() +
  geom_sf(data = co_map_data, aes(fill = total_reported_cases_first), color = "white", size = 0.2) +
  scale_fill_gradient(
    name = "Total WNV Cases",
    low = "#bcbddc",  #light purple
    high = "#3f007d", #dark purple
    trans = "log1p",
    breaks = c(1, 5, 10, 25, 50, 100),
    labels = c("1", "5", "10", "25", "50", "100+"),
    na.value = "gray90"
  ) +
  labs(title = "WNV Case Counts by County in Colorado") +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(size = 14, face = "bold"),
    legend.key.height = unit(0.8, "cm"),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10)
  )

#display the map 
print(wnv_map_co)



```



#### green space map


```{r greenspace_greens}

#prep the greenspace data to group everything by county over the study period 
greenspace_summary <- merged_by_year_CO %>%
  group_by(County_FIPS) %>%
  summarise(mean_greenspace_first = mean(greenspace, na.rm = TRUE))

#joining to spatial data (co_map_data is the shapefiles created in a chunk above)
co_map_data <- left_join(co_map_data, greenspace_summary, by = "County_FIPS")


#make the map with dark green as higher green space, less green as lower greenspace 
greenspace_map_co <- ggplot() +
  geom_sf(data = co_map_data, aes(fill = mean_greenspace_first), color = "white", size = 0.2) +
  scale_fill_gradient(
    name = "Mean Greenspace (%)",
    low = "#a1d99b",  #lighter green = less
    high = "#00441b", #dark green = more
    na.value = "gray90"
  ) +
  labs(title = "Fig 2 Mean Greenspace by County in Colorado") +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(size = 14, face = "bold"),
    legend.key.height = unit(0.8, "cm"),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10)
  )
print(greenspace_map_co)



```





#### trying an overlay of WNV hotspots and greenspace

```{r overlayWNV}

#pick out the top WNV counties over study period (top 10%)
wnv_hotspots <- co_map_data %>%
  filter(total_reported_cases_first >= quantile(total_reported_cases_first, 0.90, na.rm = TRUE)) %>%
  mutate(hotspot = TRUE)

#greenspace base map + WNV hotspot overlay
ggplot() +
  geom_sf(data = co_map_data, aes(fill = mean_greenspace_first), color = "gray80") +
  geom_sf(data = wnv_hotspots, fill = NA, color = "red", size = 1.5) +  #red outline for counties in top 10% of total WNV cases
  scale_fill_gradient(
    name = "Mean Greenspace (%)",
    low = "#a1d99b", high = "#00441b", na.value = "gray90"
  ) +
  labs(title = "WNV Hotspots Overlaid with Greenspace Map (CO)") +
  theme_minimal()


### Red outlines highlight counties in the top 10% of total West Nile Virus (WNV) cases over the study period 
# The map visually suggests that counties with higher WNV incidence do not necessarily have the highest greenspace percentages- 
#supports statistical analysis above that doesn't show a significant association between WNV and greenspace in full model

```




#### IRR map updated with offset model 

```{r IRRmapoffset}

### IRR map from offset model

#take out the betas from the offset model
coefs_offset <- tidy(CO_nb_model_interaction_offset)

beta_gs_offset <- coefs_offset %>% filter(term == "greenspace") %>% pull(estimate)
beta_int_offset <- coefs_offset %>% filter(term == "greenspace:svi_score") %>% pull(estimate)

#calculate greenspace IRR using offset model coefficients
merged_by_year_CO_IRR_offset <- merged_by_year_CO_adj %>%
  mutate(
    gs_IRR = exp(beta_gs_offset + beta_int_offset * svi_score)
  )

#aggregate everything to county level so we can display it 
county_IRRs_offset <- merged_by_year_CO_IRR_offset %>%
  group_by(County_FIPS) %>%
  summarise(gs_IRR = mean(gs_IRR, na.rm = TRUE))

#join to spatial data, map and IRRs by FIPS 
co_map_with_IRR_offset <- co_map_data %>%
  left_join(county_IRRs_offset, by = "County_FIPS")


#look at range of values to adjust legend accordingly
summary(co_map_with_IRR_offset$gs_IRR)
range(co_map_with_IRR_offset$gs_IRR)


irr_map_CO_offset <- ggplot() +
  geom_sf(data = co_map_with_IRR_offset, aes(fill = gs_IRR), color = "white", size = 0.2) +
  scale_fill_gradientn(
    name = "IRR for Greenspace",
    colors = c("#08306b", "#2171b5", "#a6c8e2", "#fddbc7", "#ef8a62", "#b2182b"),
    values = scales::rescale(c(0.95, 1.00, 1.05, 1.10, 1.15)),
    limits = c(0.95, 1.15),
    breaks = c(0.95, 1.00, 1.05, 1.10, 1.15),
    labels = c("0.95", "1.00", "1.05", "1.10", "1.15"),
    na.value = "gray90"
  ) +
  labs(title = "Figure 4. IRR for Greenspace on WNV Incidence by County (Offset Model)") +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(size = 14, face = "bold"),
    legend.key.height = unit(0.8, "cm"),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 10)
  )

#display the map 
print(irr_map_CO_offset)



```





################################################################################
# TABLES  #####################
################################################################################


#### Create table 1 descriptive statistics
#### median and IQR (25% - 75%) 

```{r Table1}

library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)

#creating Table 1 descriptive statistics 
#median and IQR (25 - 75) for each of the variables 
#taking stats from reported cases, greenspace (rescaled), pop density (rescaled), temp (rescaled), and precip 
#rounding everything to a few decimal points 
table1_vars <- merged_by_year_CO_offset %>%
  summarise(
    `Reported WNV Cases` = paste0(
      median(reported_cases, na.rm = TRUE), " (",
      quantile(reported_cases, 0.25, na.rm = TRUE), "–",
      quantile(reported_cases, 0.75, na.rm = TRUE), ")"
    ),
    `Greenspace (%)` = paste0(
      round(median(greenspace_10 * 10, na.rm = TRUE), 1), " (",
      round(quantile(greenspace_10 * 10, 0.25, na.rm = TRUE), 1), "–",
      round(quantile(greenspace_10 * 10, 0.75, na.rm = TRUE), 1), ")"
    ),
    `Population Density (per sq mi)` = paste0(
      round(median(population_density, na.rm = TRUE), 1), " (",
      round(quantile(population_density, 0.25, na.rm = TRUE), 1), "–",
      round(quantile(population_density, 0.75, na.rm = TRUE), 1), ")"
    ),
    `SVI Score` = paste0(
      round(median(svi_score + attr(merged_by_year_CO_offset$svi_score, "scaled:center"), na.rm = TRUE), 2), " (",
      round(quantile(svi_score + attr(merged_by_year_CO_offset$svi_score, "scaled:center"), 0.25, na.rm = TRUE), 2), "–",
      round(quantile(svi_score + attr(merged_by_year_CO_offset$svi_score, "scaled:center"), 0.75, na.rm = TRUE), 2), ")"
    ),
    `Mean Temperature (°F)` = paste0(
      round(median(temp_5F * 5, na.rm = TRUE), 1), " (",
      round(quantile(temp_5F * 5, 0.25, na.rm = TRUE), 1), "–",
      round(quantile(temp_5F * 5, 0.75, na.rm = TRUE), 1), ")"
    ),
    `Mean Precipitation (inches)` = paste0(
      round(median(precip_value + attr(merged_by_year_CO_offset$precip_value, "scaled:center"), na.rm = TRUE), 2), " (",
      round(quantile(precip_value + attr(merged_by_year_CO_offset$precip_value, "scaled:center"), 0.25, na.rm = TRUE), 2), "–",
      round(quantile(precip_value + attr(merged_by_year_CO_offset$precip_value, "scaled:center"), 0.75, na.rm = TRUE), 2), ")"
    )
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Median (IQR)")  #make it so that the var names are rows not columns

#make it a nice looking table with kable package to insert into paper 
kable(table1_vars, format = "html", caption = "Table 1. Descriptive Statistics for Colorado Counties (Median and IQR)",
      col.names = c("Variable", "Median (IQR)")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)



```



#### table 2 full model IRR for WNV incidence

```{r Table2_full_model}


#make the table for the full model with interaction term with IRR, CI, and p-values
table2_data <- tidy(CO_nb_model_interaction_offset, exponentiate = TRUE, conf.int = TRUE) %>%    #clean the terms up to be descriptive, not just var names from datasets 
  mutate(
    term_clean = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "pop_density_50" ~ "Population Density (per 50 people/sq mi)",
      term == "temp_5F" ~ "Temperature (per 5°F)",
      term == "precip_value" ~ "Precipitation (inches/year)",
      term == "greenspace" ~ "Greenspace (per 10%)",
      term == "svi_score" ~ "SVI Score (centered)",
      term == "greenspace:svi_score" ~ "Greenspace × SVI Interaction",
      TRUE ~ term
    ),
    IRR = sprintf("%.2f", estimate),
    CI = paste0("(", sprintf("%.2f", conf.low), "–", sprintf("%.2f", conf.high), ")"),
    `p-value` = ifelse(p.value < 0.001, "<0.001", sprintf("%.3f", p.value))    #display <0.001, sig fis ~3 
  ) %>%
  dplyr::select(Predictor = term_clean, IRR, CI, `p-value`)

#format the table with a footnote 
kable(table2_data,
      format = "html",
      caption = "Table 2. Adjusted Incidence Rate Ratios (IRRs) for Predictors of West Nile Virus (WNV) Cases, Colorado Counties") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
  footnote(general = "IRRs estimated from a negative binomial model with a log link. The model included an offset for the natural log of total county population to estimate WNV incidence. Continuous variables were centered and scaled: temperature per 5°F, population density per 50 people/sq mi, and greenspace per 10 percentage points.")

```

#### Stratified Analyses Tables

#### table 3 Low SVI (low vulnerability, less disadvantage)

```{r Table3}

#table 3: low SVI counties (less disadvantage)
table3_low_svi <- low_svi_results_offset %>%
  mutate(
    term_clean = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "greenspace" ~ "Greenspace (per 10%)",
      term == "pop_density_50" ~ "Population Density (per 50 people/sq mi)",
      term == "temp_5F" ~ "Temperature (per 5°F)",
      term == "precip_value" ~ "Precipitation (inches/year)",
      TRUE ~ term
    ),
    IRR = sprintf("%.2f", estimate),
    CI = paste0("(", sprintf("%.2f", conf.low), "–", sprintf("%.2f", conf.high), ")"),
    `p-value` = ifelse(p.value < 0.001, "<0.001", sprintf("%.3f", p.value))    #display p-values <0.001 as that 
  ) %>%
  dplyr::select(Predictor = term_clean, IRR, CI, `p-value`)

#make it a nice looking table with kable package
kable(table3_low_svi,
      format = "html",
      caption = "Table 3. Adjusted Incidence Rate Ratios (IRRs) for Predictors of West Nile Virus (WNV) in Low-SVI Counties, Colorado, 2011–2021") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
  footnote(general = "IRRs estimated from a negative binomial model with a log link, stratified by counties with below-median Social Vulnerability Index (SVI). The model includes an offset for the log of total population. Continuous predictors were centered and scaled.")



```



#### table 4 high SVI (more disadvantage) results of stratified analysis

```{r Table4}

#make the table for the high SVI (more disadvantage) results 

#table 4: for High SVI counties
table4_high_svi <- high_svi_results_offset %>%
  mutate(
    term_clean = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "greenspace" ~ "Greenspace (per 10%)",
      term == "pop_density_50" ~ "Population Density (per 50 people/sq mi)",
      term == "temp_5F" ~ "Temperature (per 5°F)",
      term == "precip_value" ~ "Precipitation (inches/year)",
      TRUE ~ term
    ),
    IRR = sprintf("%.2f", estimate),
    CI = paste0("(", sprintf("%.2f", conf.low), "–", sprintf("%.2f", conf.high), ")"),
    `p-value` = ifelse(p.value < 0.001, "<0.001", sprintf("%.3f", p.value))   #show significant p-values as <0.001 if applicable
  ) %>%
  dplyr::select(Predictor = term_clean, IRR, CI, `p-value`)

#make it display worthy - Table 4
kable(table4_high_svi,
      format = "html",
      caption = "Table 4. Adjusted Incidence Rate Ratios (IRRs) for Predictors of West Nile Virus (WNV) in High-SVI Counties, Colorado, 2011–2021") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
  footnote(general = "IRRs estimated from a negative binomial model with a log link, stratified by counties with above-median Social Vulnerability Index (SVI). The model includes an offset for the log of total population. Continuous predictors were centered and scaled.")



```






################################################################################
# FIN
################################################################################